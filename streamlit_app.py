# app.py

import os
import re
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from typing import List, TypedDict, Tuple

from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

load_dotenv()

llm_model = "gemini-2.5-flash"
embedding_model = "text-embedding-3-large"


class ContextDocument(TypedDict):
    source: str
    content: str


class ResponseDict(TypedDict):
    output_text: str
    source_documents: List[ContextDocument]


def get_conversational_chain():
    """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ëŠ” ì²´ì¸ ìƒì„±"""
    prompt_template = """ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ëŒë´„êµì‹¤, ë°©ê³¼í›„êµì‹¤, ëŠ˜ë´„êµì‹¤ ìš´ì˜ì— ê´€í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´, ê·¸ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {input}

ë‹µë³€ ì§€ì¹¨:
- ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ ë¨¼ì € í‰ê°€í•˜ì„¸ìš”
- ê´€ë ¨ ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- ê°€ë…ì„±ì„ ìœ„í•´ ì ì ˆí•œ ì¤„ë°”ê¿ˆê³¼ ë¬¸ë‹¨ì„ ì‚¬ìš©í•˜ì„¸ìš”

ë‹µë³€:"""

    model = ChatGoogleGenerativeAI(
        model=llm_model,
        temperature=0.3,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
    )

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "input"],
    )

    stuff_documents_chain = create_stuff_documents_chain(model, prompt)
    return stuff_documents_chain


def preprocess_question(question: str) -> str:
    """ì§ˆë¬¸ì„ ì „ì²˜ë¦¬í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” í•¨ìˆ˜"""

    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    question = re.sub(r"\s+", " ", question).strip()

    # ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(question) < 5:
        return question

    # ì¡´ëŒ“ë§ì„ í‰ì„œë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
    question = re.sub(r"í•´\s*ì£¼ì„¸ìš”", "í•˜ëŠ” ë°©ë²•", question)
    question = re.sub(r"ì•Œë ¤\s*ì£¼ì„¸ìš”", "", question)
    question = re.sub(r"ê°€ë¥´ì³\s*ì£¼ì„¸ìš”", "", question)

    # í•œê¸€ í‚¤ì›Œë“œ í™•ì¥ (ìˆ˜ë‹¹ ê´€ë ¨)
    keyword_expansions = {
        "ë‹´ì„ìˆ˜ë‹¹": ["ë‹´ì„ìˆ˜ë‹¹", "ë‹´ì„ ìˆ˜ë‹¹", "ë‹´ì„êµì‚¬ ìˆ˜ë‹¹", "ë‹´ì„êµì‚¬ìˆ˜ë‹¹"],
        "ë¶€ì¥ìˆ˜ë‹¹": ["ë¶€ì¥ìˆ˜ë‹¹", "ë¶€ì¥ ìˆ˜ë‹¹", "ë¶€ì¥êµì‚¬ ìˆ˜ë‹¹", "ë¶€ì¥êµì‚¬ìˆ˜ë‹¹"],
        "ìˆ˜ë‹¹": ["ìˆ˜ë‹¹", "ê¸‰ì—¬", "ë³´ìˆ˜", "ìˆ˜ë ¹"],
        "ë°›ì„ ìˆ˜ ìˆ": ["ë°›ì„ ìˆ˜ ìˆ", "ìˆ˜ë ¹ ê°€ëŠ¥", "ì§€ê¸‰", "ë°›ëŠ”"],
        "ë¶ˆê°€": ["ë¶ˆê°€", "ë¶ˆê°€ëŠ¥", "ë°›ì„ ìˆ˜ ì—†", "ìˆ˜ë ¹ ë¶ˆê°€"],
    }

    # í‚¤ì›Œë“œ í™•ì¥ ì ìš©
    expanded_question = question
    for keyword, expansions in keyword_expansions.items():
        if keyword in question:
            expanded_question += f" {' '.join(expansions)}"

    return expanded_question.strip()


def extract_keywords(question: str) -> List[str]:
    """AIë¥¼ í™œìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ê°œì„ ë¨)"""

    try:
        # ë¨¼ì € AI ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥ ì‹œë„
        expanded_keywords = get_expanded_keywords_with_gemini(question)
        if expanded_keywords:
            return expanded_keywords[:8]  # ìƒìœ„ 8ê°œ ë°˜í™˜
    except Exception as e:
        print(f"AI í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # í´ë°±: ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    return extract_basic_keywords(question)


def get_expanded_keywords_with_gemini(question: str) -> List[str]:
    """Gemini AIë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ í™•ì¥ í•¨ìˆ˜"""
    try:
        model = ChatGoogleGenerativeAI(
            model=llm_model,
            temperature=0.1,
        )

        prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ê´€ë ¨ í‚¤ì›Œë“œë“¤ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”:
        
        ì§ˆë¬¸: {question}
        
        ìš”êµ¬ì‚¬í•­:
        1. í•µì‹¬ í‚¤ì›Œë“œ 5-8ê°œë¥¼ ì¶”ì¶œ
        2. ê° í‚¤ì›Œë“œì˜ ìœ ì‚¬ì–´, ë™ì˜ì–´, ê´€ë ¨ì–´ë„ í¬í•¨
        3. ë„ì–´ì“°ê¸° ë³€í˜•ë„ ê³ ë ¤
        4. ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€
        
        í‚¤ì›Œë“œê°€ ë‹´ì„ìˆ˜ë‹¹ì¼ ë•Œ ì¶œë ¥ ì˜ˆì‹œ: ë‹´ì„ ìˆ˜ë‹¹, ë‹´ì„êµì‚¬ ìˆ˜ë‹¹, ìˆ˜ë‹¹, ê¸‰ì—¬
        """

        response = model.invoke(prompt)
        keywords = [k.strip() for k in str(response.content).split(",")]
        return keywords[:8]

    except Exception as e:
        print(f"Gemini í‚¤ì›Œë“œ í™•ì¥ ì˜¤ë¥˜: {e}")
        return []


def extract_basic_keywords(question: str) -> List[str]:
    """ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (í´ë°±ìš©)"""
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {
        "ì€",
        "ëŠ”",
        "ì´",
        "ê°€",
        "ì„",
        "ë¥¼",
        "ì—",
        "ì—ì„œ",
        "ì˜",
        "ë¡œ",
        "ìœ¼ë¡œ",
        "ì™€",
        "ê³¼",
        "í•˜ë‹¤",
        "ìˆë‹¤",
        "ì—†ë‹¤",
    }

    # ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
    words = re.findall(r"\b\w+\b", question)
    keywords = [word for word in words if word not in stopwords and len(word) > 1]

    return keywords[:5]


def keyword_text_search(db, question: str) -> List[Tuple]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ì˜ ëŒ€ì•ˆ)"""

    keywords = extract_keywords(question)
    if not keywords:
        return []

    # ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ í…ìŠ¤íŠ¸ ë§¤ì¹­
    try:
        all_docs = db.similarity_search("", k=1000)  # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°

        matching_docs = []
        for doc in all_docs:
            content = doc.page_content.lower()
            question_lower = question.lower()

            # ì •í™•í•œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in keywords:
                keyword_lower = keyword.lower()
                if keyword_lower in content:
                    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ íšŸìˆ˜ì— ë”°ë¼ ì ìˆ˜ ì¦ê°€
                    count = content.count(keyword_lower)
                    score += count * len(keyword)

            # ì›ë³¸ ì§ˆë¬¸ê³¼ì˜ ìœ ì‚¬ì„±ë„ ê³ ë ¤
            if any(word in content for word in question_lower.split()):
                score += 10

            if score > 0:
                # ì ìˆ˜ë¥¼ ìœ ì‚¬ë„ì²˜ëŸ¼ ë³€í™˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                similarity_score = min(0.95, score / 100)
                matching_docs.append(
                    (doc, 1 - similarity_score)
                )  # FAISSëŠ” ê±°ë¦¬ ê¸°ë°˜ì´ë¯€ë¡œ ì—­ë³€í™˜

        # ì ìˆ˜ìˆœ ì •ë ¬
        matching_docs.sort(key=lambda x: x[1])

        return matching_docs

    except Exception as e:
        print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def analyze_search_results(
    user_question: str, similar_docs: List[Tuple], threshold: float = 0.7
):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""

    if st.session_state.get("debug_mode", False):
        st.write(f"**ê²€ìƒ‰ ë¶„ì„ ê²°ê³¼**")
        st.write(f"ì§ˆë¬¸: {user_question}")
        st.write(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(similar_docs)}")

        relevant_docs = []
        for i, (doc, score) in enumerate(similar_docs):
            with st.expander(f"ë¬¸ì„œ {i+1} (ì ìˆ˜: {score:.3f})"):
                st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
                st.write(f"**ë‚´ìš© ê¸¸ì´:** {len(doc.page_content)}")
                st.write(
                    f"**ê´€ë ¨ì„±:** {'ë†’ìŒ âœ…' if score >= threshold else 'ë‚®ìŒ âŒ'}"
                )
                st.write(f"**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:**")
                st.text(
                    doc.page_content[:300] + "..."
                    if len(doc.page_content) > 300
                    else doc.page_content
                )

            if score >= threshold:
                relevant_docs.append((doc, score))

        st.write(f"ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")
        return relevant_docs

    # ë””ë²„ê·¸ ëª¨ë“œê°€ ì•„ë‹ ë•ŒëŠ” ì½˜ì†”ì—ë§Œ ì¶œë ¥
    print(f"\n=== ì§ˆë¬¸: {user_question} ===")
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(similar_docs)}")

    relevant_docs = []
    for i, (doc, score) in enumerate(similar_docs):
        print(f"\në¬¸ì„œ {i+1}:")
        print(f"  ì ìˆ˜: {score:.3f}")
        print(f"  ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
        print(f"  ë‚´ìš© ê¸¸ì´: {len(doc.page_content)}")
        print(f"  ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:150]}...")

        if score >= threshold:
            relevant_docs.append((doc, score))
            print("  âœ… ê´€ë ¨ì„± ë†’ìŒ")
        else:
            print("  âŒ ê´€ë ¨ì„± ë‚®ìŒ")

    print(f"\nê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")
    return relevant_docs


def check_vector_db_quality():
    """ë²¡í„° DBì˜ í’ˆì§ˆì„ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""

    if not os.path.exists("faiss_index"):
        st.error("ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    try:
        # embeddings = GoogleGenerativeAIEmbeddings(model=embedding_model)
        embeddings = OpenAIEmbeddings(model=embedding_model)
        db = FAISS.load_local(
            "faiss_index", embeddings, allow_dangerous_deserialization=True
        )

        # ë²¡í„° DB í†µê³„
        total_docs = db.index.ntotal
        st.success(f"ë²¡í„°DB ë¡œë“œ ì„±ê³µ!")
        st.write(f"**ì´ ë¬¸ì„œ ìˆ˜:** {total_docs}")

        # ìƒ˜í”Œ ë¬¸ì„œë“¤ í™•ì¸
        if total_docs > 0:
            sample_docs = db.similarity_search("", k=min(5, total_docs))
            st.write("**ìƒ˜í”Œ ë¬¸ì„œë“¤:**")
            for i, doc in enumerate(sample_docs):
                with st.expander(f"ìƒ˜í”Œ ë¬¸ì„œ {i+1}"):
                    st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
                    st.write(f"**ê¸¸ì´:** {len(doc.page_content)}")
                    st.write(f"**ë‚´ìš©:**")
                    st.text(
                        doc.page_content[:200] + "..."
                        if len(doc.page_content) > 200
                        else doc.page_content
                    )

    except Exception as e:
        st.error(f"ë²¡í„°DB ì²´í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def test_keyword_search(keyword: str):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    if not os.path.exists("faiss_index"):
        st.error("ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    try:
        # embeddings = GoogleGenerativeAIEmbeddings(model=embedding_model)
        embeddings = OpenAIEmbeddings(model=embedding_model)
        db = FAISS.load_local(
            "faiss_index", embeddings, allow_dangerous_deserialization=True
        )

        st.write(f"**'{keyword}' ê²€ìƒ‰ ê²°ê³¼:**")

        # 1. ë²¡í„° ê²€ìƒ‰
        vector_results = db.similarity_search_with_score(keyword, k=5)
        st.write(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(vector_results)}ê°œ")

        for i, (doc, score) in enumerate(vector_results):
            with st.expander(f"ë²¡í„° ê²€ìƒ‰ {i+1} (ì ìˆ˜: {score:.3f})"):
                st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
                # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ í•˜ì´ë¼ì´íŠ¸
                content = doc.page_content
                if keyword in content:
                    highlighted = content.replace(keyword, f"**{keyword}**")
                    st.markdown(highlighted)
                else:
                    st.write(content[:300] + "...")

        # 2. í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text_results = keyword_text_search(db, keyword)
        st.write(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼: {len(text_results)}ê°œ")

        for i, (doc, score) in enumerate(text_results[:5]):
            with st.expander(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ {i+1} (ì ìˆ˜: {score:.3f})"):
                st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
                content = doc.page_content
                if keyword in content:
                    highlighted = content.replace(keyword, f"**{keyword}**")
                    st.markdown(highlighted)
                else:
                    st.write(content[:300] + "...")

    except Exception as e:
        st.error(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")


def clear_chat_history():
    """ì±„íŒ… ê¸°ë¡ì„ ì§€ìš°ëŠ” í•¨ìˆ˜"""
    st.session_state.messages = [
        {"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
    ]


def user_input(user_question: str) -> ResponseDict:
    """ê°œì„ ëœ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜"""
    # embeddings = GoogleGenerativeAIEmbeddings(model=embedding_model)
    embeddings = OpenAIEmbeddings(model=embedding_model)

    if not os.path.exists("faiss_index"):
        st.error("ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"output_text": "ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", "source_documents": []}

    try:
        new_db = FAISS.load_local(
            "faiss_index", embeddings, allow_dangerous_deserialization=True
        )

        # ìœ ì‚¬ë„ ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸° (ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •)
        similarity_threshold = st.session_state.get(
            "similarity_threshold", 0.5
        )  # ê¸°ë³¸ê°’ì„ 0.5ë¡œ ë‚®ì¶¤

        # ì§ˆë¬¸ ì „ì²˜ë¦¬
        # user_question = preprocess_question(user_question)

        # ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ì ìš©
        search_results = []

        # 1. ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        original_results = new_db.similarity_search_with_score(user_question, k=5)
        search_results.extend(original_results)

        # 2. ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        if user_question != user_question:
            processed_results = new_db.similarity_search_with_score(user_question, k=5)
            search_results.extend(processed_results)

        # 3. í‚¤ì›Œë“œ ì¶”ì¶œ ê²€ìƒ‰
        keywords = extract_keywords(user_question)
        for keyword in keywords:
            keyword_results = new_db.similarity_search_with_score(keyword, k=3)
            search_results.extend(keyword_results)

        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ë¡œ ì •ë ¬
        unique_results = {}
        for doc, score in search_results:
            doc_id = f"{doc.metadata.get('source', '')}_{hash(doc.page_content)}"
            if doc_id not in unique_results or unique_results[doc_id][1] > score:
                unique_results[doc_id] = (doc, score)

        # ì ìˆ˜ìˆœ ì •ë ¬
        similar_docs = sorted(unique_results.values(), key=lambda x: x[1])[:8]

        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        relevant_docs = analyze_search_results(
            user_question, similar_docs, similarity_threshold
        )

        # ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œë„
        if not relevant_docs:
            text_search_results = keyword_text_search(new_db, user_question)
            if text_search_results:
                st.info("ğŸ’¡ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                relevant_docs = text_search_results[:3]  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©

        # ì—¬ì „íˆ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ë°˜í™˜
        if not relevant_docs:
            return {
                "output_text": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ '{user_question}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:\n- ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©\n- ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸\n- ë””ë²„ê·¸ ëª¨ë“œì—ì„œ ê²€ìƒ‰ ê³¼ì • í™•ì¸",
                "source_documents": [],
            }

        # retriever ì„¤ì • ê°œì„ 
        retriever = new_db.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": 8,
            },
        )

        # RAG ì²´ì¸ ì‹¤í–‰
        document_chain = get_conversational_chain()
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        response = retrieval_chain.invoke({"input": user_question})

        # ì°¸ê³  ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
        source_info: List[ContextDocument] = []
        for doc in response["context"]:
            if "source" in doc.metadata:
                source_info.append(
                    {
                        "source": doc.metadata["source"],
                        "content": doc.page_content,
                    }
                )

        response_text: str = response["answer"]

        return {"output_text": response_text, "source_documents": source_info}

    except Exception as e:
        st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"output_text": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", "source_documents": []}


def add_debug_sidebar():
    """ë””ë²„ê·¸ ëª¨ë“œ ì‚¬ì´ë“œë°” ì¶”ê°€"""
    with st.sidebar:
        st.header("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ")

        # ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€
        debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”", key="debug_mode")

        if debug_mode:
            st.write("---")

            # AI í‚¤ì›Œë“œ í™•ì¥ í…ŒìŠ¤íŠ¸
            st.write("**ğŸ¤– AI í‚¤ì›Œë“œ í™•ì¥ í…ŒìŠ¤íŠ¸:**")
            test_question = st.text_input(
                "í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ë‹´ì„ìˆ˜ë‹¹ì„ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?"
            )
            if st.button("AI í‚¤ì›Œë“œ í™•ì¥ í…ŒìŠ¤íŠ¸") and test_question:
                with st.spinner("AIê°€ í‚¤ì›Œë“œë¥¼ ë¶„ì„ ì¤‘..."):
                    expanded = get_expanded_keywords_with_gemini(test_question)
                    st.write(f"**í™•ì¥ëœ í‚¤ì›Œë“œ:** {', '.join(expanded)}")

            st.write("---")

            # ë²¡í„°DB í’ˆì§ˆ ì²´í¬ ë²„íŠ¼
            if st.button("ë²¡í„°DB í’ˆì§ˆ ì²´í¬"):
                check_vector_db_quality()

            # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
            similarity_threshold = st.slider(
                "ìœ ì‚¬ë„ ì„ê³„ê°’",
                min_value=0.0,
                max_value=1.0,
                value=0.5,  # ê¸°ë³¸ê°’ì„ 0.5ë¡œ ë‚®ì¶¤
                step=0.1,
                key="similarity_threshold",
                help="ì´ ê°’ë³´ë‹¤ ë‚®ì€ ìœ ì‚¬ë„ì˜ ë¬¸ì„œëŠ” ê´€ë ¨ì„±ì´ ë‚®ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.",
            )

            # íŠ¹ì • í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            st.write("**ì§ì ‘ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:**")
            test_keyword = st.text_input(
                "í…ŒìŠ¤íŠ¸í•  í‚¤ì›Œë“œ ì…ë ¥", placeholder="ì˜ˆ: ë‹´ì„ìˆ˜ë‹¹"
            )
            if st.button("í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸") and test_keyword:
                test_keyword_search(test_keyword)

            # ê²€ìƒ‰ ì„¤ì •
            st.write("**ê²€ìƒ‰ ì„¤ì •:**")
            st.write(f"- ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
            st.write(f"- ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜: 8ê°œ")
            st.write(f"- ëª¨ë¸ ì˜¨ë„: 0.3")
            st.write(f"- AI í‚¤ì›Œë“œ í™•ì¥: âœ… í™œì„±í™”")
            st.write(f"- ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ: âœ… í™œì„±í™”")

        return debug_mode


def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    st.set_page_config(
        page_title="ëŠ˜ë´„í•™êµ ìš´ì˜ ë„ìš°ë¯¸ ì±—ë´‡", page_icon="nb_small.png", layout="wide"
    )

    # ë””ë²„ê·¸ ì‚¬ì´ë“œë°” ì¶”ê°€
    debug_mode = add_debug_sidebar()

    # ë©”ì¸ í—¤ë”
    col1, col2 = st.columns([1, 8])
    with col1:
        if os.path.exists("nb_small.png"):
            st.image("nb_small.png", use_container_width=True)
    with col2:
        st.markdown(
            """
            <div style="display:flex; align-items:center; height:54px;">
                <span style="font-size:2.2em; font-weight:bold; height:54px;">ëŠ˜ë´„í•™êµ ìš´ì˜ ë„ìš°ë¯¸ ì±—ë´‡</span>
            </div>
            """,
            unsafe_allow_html=True,
        )

    st.write("ë¬¸ì˜ì‚¬í•­ ë° ì˜¤ë¥˜ë³´ê³ : í¬í•­ì›ë™ì´ˆë“±í•™êµ êµì‚¬ ê¹€ì§€ì›")

    # ë””ë²„ê·¸ ëª¨ë“œ í‘œì‹œ
    if debug_mode:
        st.warning(
            "ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê³¼ì •ì´ ìƒì„¸íˆ í‘œì‹œë©ë‹ˆë‹¤."
        )

    # ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸° ë²„íŠ¼
    if st.button("ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸°", key="clear_chat"):
        clear_chat_history()
        st.rerun()

    # ì±„íŒ… ë©”ì‹œì§€ ì´ˆê¸°í™”
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            }
        ]

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ì‘ë‹µ ìƒì„±
                response_dict = user_input(prompt)

                # ì‘ë‹µ í‘œì‹œ
                if response_dict and "output_text" in response_dict:
                    full_response = response_dict["output_text"]
                    st.markdown(full_response)

                    # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                    if (
                        response_dict.get("source_documents")
                        and len(response_dict["source_documents"]) > 0
                    ):

                        st.markdown("---")
                        st.markdown("**ğŸ“š ì°¸ê³  ë¬¸ì„œ:**")

                        # ì†ŒìŠ¤ ë¬¸ì„œë¥¼ íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
                        source_groups = {}
                        for doc in response_dict["source_documents"]:
                            source = doc.get("source", "Unknown")
                            if source not in source_groups:
                                source_groups[source] = []
                            source_groups[source].append(doc.get("content", ""))

                        # ê° íŒŒì¼ë³„ë¡œ expander ìƒì„±
                        for source, contents in source_groups.items():
                            with st.expander(f"ğŸ“„ {source} ({len(contents)}ê°œ ì„¹ì…˜)"):
                                for i, content in enumerate(contents, 1):
                                    st.write(f"**ì„¹ì…˜ {i}:**")
                                    st.write(content)
                                    if i < len(contents):
                                        st.write("---")

                    # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.messages.append(
                        {"role": "assistant", "content": full_response}
                    )

                else:
                    error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    st.error(error_message)
                    st.session_state.messages.append(
                        {"role": "assistant", "content": error_message}
                    )


if __name__ == "__main__":
    main()
