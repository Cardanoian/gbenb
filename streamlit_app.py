import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

load_dotenv()


def get_conversational_chain():
    prompt_template = """ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ëŒë´„êµì‹¤, ë°©ê³¼í›„êµì‹¤, ëŠ˜ë´„êµì‹¤ ìš´ì˜ì— ê´€í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹´ë‹¹ êµì‚¬ë“¤ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ê°€ë…ì„±ì„ ìœ„í•´ ì ì ˆí•œ ì¤„ë°”ê¿ˆê³¼ ë¬¸ë‹¨ êµ¬ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ëª¨ë“  ê´€ë ¨ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ë§Œì•½ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ìœ¼ë©´, 'ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë§í•˜ì‹­ì‹œì˜¤.
ì»¨í…ìŠ¤íŠ¸:\n {context}\nì§ˆë¬¸: \n{input}\n\në‹µë³€:
"""

    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.2,
    )
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "input"],
    )
    stuff_documents_chain = create_stuff_documents_chain(model, prompt)
    return stuff_documents_chain


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
    ]


def user_input(user_question):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"
    )  # type: ignore

    # FAISS ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists("faiss_index"):
        st.error("ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"output_text": "ë²¡í„°DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}

    new_db = FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )
    retriever = new_db.as_retriever()

    document_chain = get_conversational_chain()

    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    response = retrieval_chain.invoke({"input": user_question})

    source_info = []
    for doc in response["context"]:
        if "source" in doc.metadata and "page" in doc.metadata:
            source_info.append(
                f"{doc.metadata['source']} (í˜ì´ì§€: {doc.metadata['page']})"
            )

    # ë‹µë³€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê³ , ì°¸ê³  ë¬¸ì„œëŠ” ë³„ë„ì˜ í‚¤ë¡œ ë°˜í™˜
    response_text = response["answer"]

    # print(response_text)
    return {"output_text": response_text, "source_documents": source_info}


def main():
    st.set_page_config(page_title="ëŠ˜ë´„í•™êµ ìš´ì˜ ë„ìš°ë¯¸ ì±—ë´‡", page_icon="ğŸ’¬")

    st.title("ëŠ˜ë´„í•™êµ ìš´ì˜ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ’¬")
    st.write("ë¬¸ì˜ì‚¬í•­ ë° ì˜¤ë¥˜ë³´ê³ ëŠ” í¬í•­ì›ë™ì´ˆë“±í•™êµ êµì‚¬ ê¹€ì§€ì›ì—ê²Œ í•´ì£¼ì„¸ìš”.")

    st.button("ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸°", on_click=clear_chat_history)

    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            }
        ]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                response_dict = user_input(prompt)
                placeholder = st.empty()
                full_response = ""
                # user_input í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ì˜ 'output_text' í‚¤ë¥¼ ì‚¬ìš©
                if response_dict and "output_text" in response_dict:
                    full_response = response_dict["output_text"]
                    placeholder.markdown(full_response)
                else:
                    full_response = "ì˜¤ë¥˜: ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    placeholder.markdown(full_response)

                # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ ë¡œì§ ì¶”ê°€
                if (
                    response_dict
                    and "source_documents" in response_dict
                    and response_dict["source_documents"]
                ):
                    st.markdown("---")  # êµ¬ë¶„ì„  ì¶”ê°€
                    st.markdown("**ì°¸ê³  ë¬¸ì„œ:**")
                    unique_sources = sorted(
                        list(set(response_dict["source_documents"]))
                    )
                    for source in unique_sources:
                        st.info(source)  # ê° ë¬¸ì„œë¥¼ st.info ìƒìì— í‘œì‹œ

        if response_dict is not None:
            # ë©”ì‹œì§€ ì €ì¥ ì‹œ ì°¸ê³  ë¬¸ì„œëŠ” ì œì™¸í•˜ê³  ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì €ì¥
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)

        if response_dict is not None:
            # ë©”ì‹œì§€ ì €ì¥ ì‹œ ì°¸ê³  ë¬¸ì„œëŠ” ì œì™¸í•˜ê³  ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì €ì¥
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)


if __name__ == "__main__":
    main()
