import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import streamlit as st
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.chains import create_retrieval_chain  # ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
from langchain.chains.combine_documents import (
    create_stuff_documents_chain,
)  # ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

load_dotenv()
# read all pdf files and return text


def get_pdf_text(pdf_docs):
    documents = []
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text()
            if text:
                documents.append(
                    Document(
                        page_content=text, metadata={"source": pdf.name, "page": i + 1}
                    )
                )
    return documents


# split text into chunks


def get_text_chunks(documents):  # ì¸ìë¥¼ documentsë¡œ ë³€ê²½
    splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    chunks = splitter.split_documents(documents)  # split_text ëŒ€ì‹  split_documents ì‚¬ìš©
    return chunks  # list of Document objects


# get embeddings for each chunk


def get_vector_store(chunks):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"
    )  # type: ignore
    vector_store = FAISS.from_documents(
        chunks, embedding=embeddings
    )  # from_texts ëŒ€ì‹  from_documents ì‚¬ìš©
    vector_store.save_local("faiss_index")


def get_conversational_chain():
    prompt_template = """ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê²Œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³ , ëª¨ë“  ì„¸ë¶€ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. 
ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë§í•˜ê³ , ì˜ëª»ëœ ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
ì»¨í…ìŠ¤íŠ¸:\n {context}\nì§ˆë¬¸: \n{input}\n\në‹µë³€:
"""

    model = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0.3,
    )
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "input"],  # "question" ëŒ€ì‹  "input"ìœ¼ë¡œ ë³€ê²½
    )
    # load_qa_chain ëŒ€ì‹  create_stuff_documents_chain ì‚¬ìš©
    stuff_documents_chain = create_stuff_documents_chain(model, prompt)
    return stuff_documents_chain  # ë³€ê²½ëœ ì²´ì¸ ë°˜í™˜


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}
    ]


def user_input(user_question):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"
    )  # type: ignore

    new_db = FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )
    retriever = new_db.as_retriever()  # ê²€ìƒ‰ê¸° ìƒì„±

    document_chain = get_conversational_chain()  # ë¬¸ì„œ ì²´ì¸ ê°€ì ¸ì˜¤ê¸°

    # ê²€ìƒ‰ ì²´ì¸ ìƒì„±
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    # invoke ë©”ì„œë“œ ì‚¬ìš©
    response = retrieval_chain.invoke({"input": user_question})

    # ì—¬ê¸°ì— ë¬¸ì„œ ì¶œì²˜ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    source_info = []
    # responseì—ì„œ docsë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ ë³€ê²½
    for doc in response["context"]:
        if "source" in doc.metadata and "page" in doc.metadata:
            source_info.append(
                f"{doc.metadata['source']} (í˜ì´ì§€: {doc.metadata['page']})"
            )

    if source_info:
        unique_sources = sorted(list(set(source_info)))
        response_text = (
            response["answer"] + "\n\nì°¸ê³  ë¬¸ì„œ: " + "\n".join(unique_sources)
        )
    else:
        response_text = response["answer"]

    print(response_text)
    return {"output_text": response_text}


def main():
    st.set_page_config(page_title="ì œë¯¸ë‹ˆ PDF ì±—ë´‡", page_icon="ğŸ¤–")

    # Sidebar for uploading PDF files
    with st.sidebar:
        st.title("ë©”ë‰´:")
        pdf_docs = st.file_uploader(
            "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ì œì¶œ ë° ì²˜ë¦¬' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”",
            accept_multiple_files=True,
        )
        if st.button("ì œì¶œ ë° ì²˜ë¦¬"):
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                documents = get_pdf_text(pdf_docs)  # raw_text ëŒ€ì‹  documentsë¡œ ë³€ê²½
                text_chunks = get_text_chunks(
                    documents
                )  # raw_text ëŒ€ì‹  documentsë¡œ ë³€ê²½
                get_vector_store(text_chunks)
                st.success("ì™„ë£Œ")

    # Main content area for displaying chat messages
    st.title("ì œë¯¸ë‹ˆë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ê³¼ ì±„íŒ…í•˜ê¸°ğŸ¤–")
    st.write("ì±„íŒ…ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.sidebar.button("ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸°", on_click=clear_chat_history)

    # Chat input
    # Placeholder for chat messages

    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {"role": "assistant", "content": "PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}
        ]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # Display chat messages and bot response
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                response_dict = user_input(
                    prompt
                )  # user_inputì´ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ë³€ìˆ˜ëª… ë³€ê²½
                placeholder = st.empty()
                full_response = ""
                for item in response_dict[
                    "output_text"
                ]:  # response_dictì—ì„œ output_textë¥¼ ê°€ì ¸ì˜´
                    full_response += item
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        if response_dict is not None:  # response_dictë¡œ ë³€ê²½
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)


if __name__ == "__main__":
    main()
